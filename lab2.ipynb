{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pui-sum-rv/lab2/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "# Lab 2 - Color Spaces and Thresholding\n",
        "\n",
        "These laboratory excersises are solved on Google Colab and are save on GitHub repo that is connected to GitHub Classroom.\n",
        "\n",
        "## Tools You need to use to Submit Assignments\n",
        "\n",
        "In this document, you will solve tasks. This is a Jupyter Notebook which has the **.ipynb** extension, is an interactive web environment for data analysis, visualization, solution presentations, education, and more.\n",
        "\n",
        "**Google Colab** is a tool that allows you to run and share Jupyter Notebook files on Google's servers, including the use of Google's CPU, GPU, and TPU resources. Colab is like Google Docs for Jupyter Notebooks. **Google Colab does not automatically save your assignment to GitHub.**\n",
        "\n",
        "**You use GitHub to save and submit your assignments.** When you accept the assignment through GitHub Classroom, a repository is automatically created on your GitHub account with a copy of the task. This is where you will save your solutions. Saving your solutions submits the tasks for that lab.\n",
        "\n",
        "## How to Solve the Tasks?\n",
        "1. Accept the task via the Google Classroom link that you will receive. Google Classroom will create a repository on your account.\n",
        "2. Go to the newly created repository on your account and click on the .ipynb file, then click Open in Colab.\n",
        "3. You will solve the tasks in Google Colab.\n",
        "\n",
        "## How to Save (Submit) Tasks?\n",
        "\n",
        "1. In Google Colab, click on the Open settings gear icon in the top-right corner.\n",
        "2. Click on the GitHub tab and check the box for Access private repositories and organizations.\n",
        "3. A new window will open for you to grant access to GitHub. For ferit-osirv, click Grant.\n",
        "4. Save and exit the settings.\n",
        "5. Click on File > Save a copy in GitHub.\n",
        "6. Select the lab repository that includes your name.\n",
        "\n",
        "> *Note:* You only need to complete steps 1-4 the first time.\n",
        "\n",
        "7. Click on **File > Save a copy in GitHub**.\n",
        "8. Select created repository **koji uključuje vaše ime**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feUPz7IDCbDx"
      },
      "source": [
        "## Copying Files from the GitHub Repository\n",
        "\n",
        "For completing the exercises, you will need images and other files that will be stored in the GitHub repository of the exercise. A command like this will be available in the notebook for each exercise. It will copy the files from GitHub to the Google Colab environment.\n",
        "\n",
        "**You need to run this command before starting each exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpP_i0KgCefb",
        "outputId": "e08a0979-cb3c-41f4-be61-07957597ae31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'clone'...\n",
            "remote: Repository not found.\n",
            "fatal: repository 'https://github.com/ferit-osirv/lab2/' not found\n"
          ]
        }
      ],
      "source": [
        "!rm -rf clone && git clone https://github.com/pui-sum-rv/lab2 clone && cp -a clone/. ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPg8Vf9Cr8D"
      },
      "source": [
        "**Google Colab will occasionally delete all files**. Therefore, you might need to rerun this command between sessions. If you encounter errors indicating that files do not exist, try running the command again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EXS_YJC2WsWD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Color spaces\n",
        "\n",
        "In this lab, you'll get familiar with image color spaces. On the web and in general usage, most images are encoded as **RGB**: **R**ed, **G**reen, and **B**lue. OpenCV generally uses **BGR**: Blue, Green, Red.\n",
        "\n",
        "This is just one of the many ways we can represent an image. In an RGB image, we get a pixel by mixing the three colors. We can get the same pixel by using different numbers and formulae to combine them. For instance, the **CMYK** color space encodes each pixel in 4 primary colors: **C**yan, **M**agenta, **Y**ellow and **K**ey (Black). Since printers use these primary colors, CMYK is often used when preparing images for print.\n",
        "\n",
        "Not all color spaces consist only of primary colors. For instance, **HSV** (**H**ue, **S**aturation, **V**alue) stores the color in Hue, the color's intensity in Saturation, and the general brightness of that pixel in Value. The Hue portion is a number in [0, 179] (in OpenCV, usually it's an arc around a circle, so [0, 360)) where 0 is red, and the hue slowly shifts to green and then blue as you get to higher numbers.\n",
        "\n",
        "![](hsv_1.png)\n",
        "\n",
        "You can think of the whole HSV color space as a cylinder. The height on the cylinder corresponds to how dark the pixel is, the distance from the center tells you how non-gray it is, and the angle tells you which color the pixel is.\n",
        "\n",
        "\n",
        "There are many color spaces each with its uses. One other color space we'll mention in **YCbCr**. Y is the **luma** component, similar to the Value in HSV. Cb is the **blue-difference chroma component**, i.e. how blue should this pixel be tinted. Similarly, Cr is the **red-difference chroma component**, which tells you how much should a pixel be tinted red. Even with a different type of representation, each YCbCr is capable of showing all RGB images.\n",
        "\n",
        "![](ycbcr.png)\n",
        "\n",
        "*(source: https://en.wikipedia.org/wiki/YCbCr#/media/File:CCD.png)*\n",
        "\n",
        "The reason YCbCr is important is because of the human eye. Our eyes are much more sensitive to luminance than actual color differences. Therefore, when compressing images, it's better to compress the chroma components than luminance if you want the image to look the same to a human observer.  This is called **chroma subsampling** and is used heavily in image and video compression, including MPEG, JPEG, DVD and Blu-Rays, and many others.\n",
        "\n",
        "## In OpenCV\n",
        "\n",
        "OpenCV supports a plethora of color spaces for images. The main function to convert color spaces is: [img = cv.cvtColor(img, code)](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab). The `code` tells OpenCV **from** which format to convert the image, as well as **to** which format. You can see all the color conversion codes [here](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga4e0972be5de079fed4e3a10e24ef5ef0). For example:\n",
        "\n",
        "- cv.COLOR_BGR2YCrCb (BGR to YCbCr)\n",
        "- cv.COLOR_YCrCb2BGR (back to BGR as the name suggests)\n",
        "- cv.COLOR_RGB2HSV\n",
        "- etc.\n",
        "\n",
        "Note: You'll have to convert the image back to RGB if you want to use matplotlib to display it in its original form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1\n",
        "\n",
        "Load the image images/peppers.png using OpenCV and convert it to the HSV color space using the aforementioned function. Then, add 30 to the H (hue) channel of the HSV image for each pixel. Convert that image back to RGB and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "\n",
        "Load the image `images/peppers.png` using OpenCV and convert it to the HSV color space using the aforementioned function. Then, set the H (hue) channel of the HSV image to 0 for each pixel. Convert that image back to RGB and display it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thresholding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Thresholding?\n",
        "\n",
        "The simplest segmentation method\n",
        "\n",
        "Application example: Separate out regions of an image corresponding to\n",
        "objects which we want to analyze. This separation is based on the variation\n",
        "of intensity between the object pixels and the background pixels.\n",
        "\n",
        "To differentiate the pixels we are interested in from the rest (which will\n",
        "eventually be rejected), we perform a comparison of each pixel intensity\n",
        "value with respect to a threshold (determined according to the problem to\n",
        "solve).\n",
        "\n",
        "Once we have separated properly the important pixels, we can set them with\n",
        "a determined value to identify them (i.e. we can assign them a value of 0\n",
        "(black), 255 (white) or any value that suits your needs).\n",
        "\n",
        "![](https://docs.opencv.org/2.4/_images/Threshold_Tutorial_Theory_Example.jpg)\n",
        "\n",
        "\n",
        "## Simple Thresholding\n",
        "\n",
        "Here, the matter is straight forward. If pixel value is greater than a\n",
        "threshold value, it is assigned one value (may be white), else it is assigned\n",
        "another value (may be black). The function used is `cv2.threshold`. First\n",
        "argument is the source image, which should be a grayscale image. Second\n",
        "argument is the threshold value which is used to classify the pixel values.\n",
        "Third argument is the ` maxVal ` which represents the value to be given if pixel\n",
        "value is more than (sometimes less than) the threshold value. OpenCV provides\n",
        "different styles of thresholding and it is decided by the fourth parameter of\n",
        "the function. Different types are:\n",
        "\n",
        "- cv2.THRESH_BINARY\n",
        "- cv2.THRESH_BINARY_INV\n",
        "- cv2.THRESH_TRUNC\n",
        "- cv2.THRESH_TOZERO\n",
        "- cv2.THRESH_TOZERO_INV\n",
        "\n",
        "To illustrate how these thresholding processes work, let’s consider that we\n",
        "have a source image with pixels with intensity values $` src(x,y) `$. \n",
        "The plot below\n",
        "depicts this. The horizontal blue line represents the threshold $` thresh `$ (fixed).\n",
        "\n",
        "![](https://docs.opencv.org/2.4/_images/Threshold_Tutorial_Theory_Base_Figure.png)\n",
        "\n",
        "The documentation clearly explains what each type is meant for. [Please check out the\n",
        "documentation](http://docs.opencv.org/doc/tutorials/imgproc/threshold/threshold.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3\n",
        "\n",
        "Using OpenCV, load the image `images/apple.jpg` as a **grayscale** image. Apply simple **binary** thresholding in two ways: 1) using the aforementioned OpenCV function and 2) using numpy by setting all pixel values above a certain threshold to 255 and the rest to 0. Display the thresholded image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# opencv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Otsu Binarization\n",
        "\n",
        "**Binarization** of an image is the process of converting the image into a format where each pixel can only be one of two possible values. For `uint8` images, these values are typically `0` (black) and `255` (white). For `float` images, the values are `0` (black) or `1.0` (white). **Binarization** is often a precursor to **thresholding**, where an image is divided into completely white and black regions, and then only the parts of the original image corresponding to completely white regions in the binary image are retained. Mathematically, by multiplying the original and binary images, the pixels that are completely white in the binary image remain unchanged, while those that are completely black are multiplied by 0 and thus will be completely black in the product image.\n",
        "\n",
        "In the previous example, you manually set the threshold. Otsu's binarization is a more advanced method that, based on the **histogram** of the image, determines the optimal threshold that best separates the pixels. A histogram is a graph that shows the number of occurrences of each value in a dataset. In the case of an image, the histogram shows for each color value how many pixels have that color.\n",
        "\n",
        "Let's look at the histogram of the `apple.jpg` image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "img = cv.imread('images/apple.jpg', cv.IMREAD_GRAYSCALE)\n",
        "plt.hist(img.flatten(), bins=256, range=(0, 255))\n",
        "plt.ylim([0, 10000])\n",
        "plt.xlim([0, 255])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the histogram, it's evident that most of the pixels are clustered around values of 255 and 50. Visually, we can see that an optimal way to separate the pixels into two groups would be with a threshold between 150 and 200, as this threshold effectively separates the two largest pixel clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4\n",
        "\n",
        "According to the following link, implement Otsu binarization for the image `apple.jpg`. Display the resulting binary image **using matplotlib**. Print the optimal threshold value according to Otsu's method to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Such a binary image can be used as a **mask** for the original image. A mask is a binary image that has a value of `0` for all pixels that should not be visible, and a maximum value (1.0 or 255) for pixels that should be visible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5\n",
        "\n",
        "Using the Otsu binary image as a mask, apply the function `img_thresholded = cv2.bitwise_and(img, img, mask=mask)` where `img` is the original grayscale image of the apple, and `mask` is the Otsu binary image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPdsk5aJ45he4xMUxx8bkpd",
      "include_colab_link": true,
      "name": "lab1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
